{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -N \"https://cainvas-static.s3.amazonaws.com/media/user_data/cainvas-admin/insect_bite.zip\"\n",
    "# !unzip -qo insect_bite.zip\n",
    "# !rm insect_bite.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'insect bite'\n",
    "\n",
    "print(\"Number of samples in - \")\n",
    "for f in os.listdir(data_dir + '/'):\n",
    "    if os.path.isdir(data_dir + '/' + f):\n",
    "        print('\\n'+f.upper())\n",
    "        for fx in os.listdir(data_dir + '/' + f + '/'):\n",
    "            print(fx, \" : \", len(os.listdir(data_dir + '/' + f +'/' + fx + '/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 8\n",
    "\n",
    "print(\"Train dataset\")\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir+'/train', batch_size=batch)\n",
    "\n",
    "print(\"Validation dataset\")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir+'/validation', batch_size=batch)\n",
    "\n",
    "print(\"Test dataset\")\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir+'/test', batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 3\n",
    "\n",
    "for x in class_names:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    filenames = os.listdir(data_dir + '/train/' + x)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        ax = plt.subplot(1, num_samples, i + 1)\n",
    "        img = Image.open(data_dir + '/train/' + x + '/' + filenames[i])\n",
    "        plt.imshow(img)\n",
    "        plt.title(x)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Looking into the shape of images and labels in one batch\\n\")  \n",
    "\n",
    "for image_batch, labels_batch in train_ds:\n",
    "    input_shape = image_batch[0].shape\n",
    "    print(\"Shape of images input for one batch: \", image_batch.shape)\n",
    "    print(\"Shape of images labels for one batch: \", labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),    # Flip along both axes\n",
    "        tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),    # Randomly zoom images in dataset\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation((-0.1, 0.1))\n",
    "    ])\n",
    "\n",
    "\n",
    "print(\"Train size (number of batches) before augmentation: \", len(train_ds))\n",
    "\n",
    "# Apply only to train set    \n",
    "aug_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "print(\"Size (number of batches) of augmented dataset: \", len(aug_ds))\n",
    "\n",
    "#Adding to train_ds\n",
    "train_ds = train_ds.concatenate(aug_ds)\n",
    "\n",
    "print(\"Train size (number of batches) after augmentation: \", len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.VGG16(weights='imagenet', input_shape=input_shape, include_top=False)    # False, do not include the classification layer of the model\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = tf.keras.layers.Dense(len(class_names), activation = 'softmax')(x)    # Add own classififcation layer\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "cb = [EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True)]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "\n",
    "history1 = model.fit(train_ds, validation_data =  val_ds, batch_size = 32, epochs=100, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "history2 = model.fit(train_ds, validation_data =  val_ds, batch_size = 32, epochs=100, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history1, history2, variable1, variable2):\n",
    " \n",
    "    var1_history = history1[variable1]\n",
    "    var1_history.extend(history2[variable1])\n",
    "    \n",
    "    var2_history = history1[variable2]\n",
    "    var2_history.extend(history2[variable2])\n",
    "  \n",
    "    plt.plot(range(len(var1_history)), var1_history)\n",
    "    plt.plot(range(len(var2_history)), var2_history)\n",
    "    plt.legend([variable1, variable2])\n",
    "    plt.title(variable1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history1.history, history2.history, \"accuracy\", 'val_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history1.history, history2.history, \"loss\", 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in test_ds.as_numpy_iterator():\n",
    "    img, label = i  \n",
    "    for x in range(len(label)):  \n",
    "        ax = plt.subplot(1, len(label), x + 1)\n",
    "        plt.axis('off') \n",
    "        plt.imshow(img[x])   \n",
    "        output = model.predict(np.expand_dims(img[x],0))   \n",
    "        pred = np.argmax(output[0])   \n",
    "        t = \"Prdicted: \" + class_names[pred] \n",
    "        t = t + \"\\nTrue: \" + class_names[label[x]]\n",
    "        t = t + \"\\nProbability: \" + str(output[0][pred])\n",
    "        plt.title(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('insect.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(\"insect.h5\",'r')\n",
    "for item in f.keys():\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"insect.h5\",'r')\n",
    "for item in f.require_group('model_weights').keys():\n",
    "    print ('model_weights/' +item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
